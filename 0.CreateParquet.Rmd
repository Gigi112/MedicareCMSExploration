---
title: "index"
author: "Dan Weinberger"
date: '2022-12-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(dplyr)
library(ggplot2)

source('./R/schema.R')
 
```

This Rmd file converts CSV files to parquet databases using the arrow package. 

denom files for 2014-2016; after 2016, shifts to MBSF files

```{r}
csv_file_denom <- "T:/denom/den_saf_lds_100_201.csv"

dest_denom <- "T:/parquet/denom/" 

#double()
#string()
#float()
#int32()
#https://www.cms.gov/Research-Statistics-Data-and-Systems/Files-for-Order/LimitedDataSets/Downloads/SAFldsDenomNov2009.pdf

csv_stream_denom <- open_dataset("T:/denom/", format = "csv", schema = sch_denom)
```

Write the data to a parquet database
```{r, eval=F}
write_dataset(csv_stream_denom, dest_denom, format = "parquet", 
              max_rows_per_file=1000000L,
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Do the same for MBSF data

Note on partioning: 
"Avoid files smaller than 20MB and larger than 2GB.
Avoid partitioning layouts with more than 10,000 distinct partitions."
https://ursalabs.org/arrow-r-nightly/articles/dataset.html

```{r, eval=F}
dest_mbsf <- "T:/parquet/mbsf/" 

csv_stream_mbsf <- open_dataset("T:/mbsf/", format = "csv", schema = sch_dbsf) %>%
  group_by(REFERENCE_YEAR) #ensure chunks are not too large for memory

write_dataset(csv_stream_mbsf, dest_mbsf, format = "parquet", 
               max_rows_per_file = 5e5,
              max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

Inpatient base files

```{r}
dest_inp <- "T:/parquet/inp/" 

csv_stream_inp <- open_dataset("T:/inp/inp1/basek2016_2021", format = "csv", schema = sch_inp_basek) %>%
  group_by(ADMSN_DT) #ensure chunks are not too large for memory

write_dataset(csv_stream_inp, dest_inp, format = "parquet", 
               max_rows_per_file = 1e5,
              #max_open_files=20, #default is 900L
              hive_style = TRUE,
              existing_data_behavior = "overwrite")
```

## Read in the parquet database
```{r}
pq_stream_denom <- open_dataset("T:/parquet/denom/", format = "parquet", schema = sch_denom)
```

## Test databases 

Take a test pull from the dataset
```{r}
test1 <- pq_stream %>% 
  slice_head( n = 10000) %>%
  collect() #use collect() to take parquet to dataframe
```

Look at age distribution of all enrollees in 2015
should be ~54 million enrollees in 2015 (58.6 million in 2021)..

to run on the csv_stream takes 151 seconds
to run on the parquet data stream takes 22 seconds
```{r}
ptm <- proc.time()
N_age2 <- pq_stream_denom %>% 
  select(AGE,RFRNC_YR) %>%
  filter(RFRNC_YR==15) %>%
group_by(AGE) %>%
  summarize(N_obs = n()) %>% #use collect() to take parquet to dataframe
    collect()

proc.time() - ptm

ggplot(N_age2, aes(x=AGE, y=N_obs)) +
  geom_line()
```

```{r}
ptm <- proc.time()
N_age_die <- pq_stream_denom %>%
  filter(DEATH_DT != '') %>%
  select(AGE,RFRNC_YR) %>%
  filter(RFRNC_YR==15) %>%
group_by(AGE) %>%
  summarize(N_deaths = n()) %>% #use collect() to take parquet to dataframe
    collect()

proc.time() - ptm

ggplot(N_age_die, aes(x=AGE, y=N_deaths)) +
  geom_line()
```

probability of death per year
```{r}
death_rate <- N_age2 %>%
  filter(AGE>=65) %>%
  left_join(N_age_die, by='AGE') %>%
  mutate( N_deaths = ifelse(is.na(N_deaths),0,N_deaths),
    death_rate= N_deaths/N_obs 
    ) 

  ggplot( death_rate, aes(x=AGE, y=death_rate)) +
  geom_line() +
    theme_classic()

```





 

